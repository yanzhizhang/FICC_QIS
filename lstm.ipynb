{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ba2d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "import pytz\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as pl\n",
    "from matplotlib.patheffects import Normal\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from future_arb.reinforcement_learning.rl_lstm import SpreadTradingEnv, LSTMActorCritic, PPO_LSTM\n",
    "from helper.future_price_retriever import FuturePriceRetriever\n",
    "from helper.spread_data_processor import SpreadDataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4140240",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    # 加载数据\n",
    "    start_date = \"20140601\"\n",
    "    end_date = dt.datetime.now(pytz.timezone(\"Asia/Shanghai\")).strftime(\"%Y%m%d\")\n",
    "\n",
    "    # 获取价差数据\n",
    "    trading_pair = [\"RB\", \"HC\"]\n",
    "    future_price_retriever = FuturePriceRetriever(start_date=start_date)\n",
    "    rb_hc_day_spread_df = future_price_retriever.get_spread_data(trading_pair, frequency=\"1d\")\n",
    "\n",
    "    # 计算技术指标\n",
    "    lockback_periods = [2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377]\n",
    "    hist_vol_windows = [2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377]\n",
    "\n",
    "    data_processor = SpreadDataProcessor()\n",
    "    rb_hc_fibo_z_score = data_processor.compute_moving_statistics(rb_hc_day_spread_df, target_col=\"RB_HC_spread\", window=lockback_periods)\n",
    "    rb_hc_fibonacci_spread_df = data_processor.compute_historical_volatility(\n",
    "        rb_hc_fibo_z_score, price_cols=[\"RB_prices\", \"HC_prices\"], window=hist_vol_windows\n",
    "    )\n",
    "    \n",
    "    # 将数据输出到本地CSV文件\n",
    "    rb_hc_fibonacci_spread_df.to_csv(\"rb_hc_fibonacci_spread.csv\", index=False)\n",
    "\n",
    "    # 从本地CSV文件读取数据\n",
    "    rb_hc_fibonacci_spread_df = pd.read_csv(\"rb_hc_fibonacci_spread.csv\")\n",
    "\n",
    "    # 初始化环境\n",
    "    env = SpreadTradingEnv(\n",
    "        rb_hc_fibonacci_spread_df,\n",
    "        init_balance=1e8,  # 初始资金\n",
    "        contract_size=10,  # 每手10吨\n",
    "        min_lots=1,  # 最小交易1手\n",
    "        lookback_window=371,\n",
    "        transaction_cost=10,  # 每手 10 元手续费\n",
    "        slippage=100,  # 每手 100 元滑点\n",
    "        cost_penalty_ratio=10,\n",
    "        drawdown_penalty_ratio=10,\n",
    "    )\n",
    "    state_dim = len(env._get_state())\n",
    "    action_dim = 2\n",
    "    hidden_dim = 32\n",
    "\n",
    "    # 初始化LSTM-PPO\n",
    "    agent = PPO_LSTM(state_dim, action_dim, hidden_dim)\n",
    "\n",
    "    # 训练参数\n",
    "    episodes = 2000\n",
    "    max_steps = 5000\n",
    "    batch_size = 64\n",
    "    seq_length = 120  # LSTM序列长度\n",
    "\n",
    "    # 训练记录\n",
    "    training_logs = {\n",
    "        \"episode\": [],\n",
    "        \"avg_reward\": [],\n",
    "        \"total_return\": [],\n",
    "        \"max_drawdown\": [],\n",
    "        \"sharpe_ratio\": [],\n",
    "        \"highest_return\": [],\n",
    "    }\n",
    "\n",
    "    print(\"开始训练\")\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_states = []\n",
    "        episode_actions = []\n",
    "        episode_rewards = []\n",
    "        episode_dones = []\n",
    "        episode_values = []\n",
    "        episode_log_probs = []\n",
    "\n",
    "        # 初始化LSTM隐藏状态\n",
    "        h = torch.zeros(1, 1, agent.policy.lstm.hidden_size)\n",
    "        c = torch.zeros(1, 1, agent.policy.lstm.hidden_size)\n",
    "        hidden = (h, c)\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # 将状态转换为序列格式\n",
    "            if len(episode_states) >= seq_length:\n",
    "                state_seq = np.array(episode_states[-seq_length:])\n",
    "            else:\n",
    "                state_seq = np.array([state] * seq_length)\n",
    "\n",
    "            # 选择动作\n",
    "            state_tensor = torch.FloatTensor(state_seq).unsqueeze(0)  # 添加batch维度\n",
    "            action_mean, value, hidden = agent.policy(state_tensor, hidden)\n",
    "            dist = Normal(action_mean, torch.ones_like(action_mean) * 0.1)  # 添加探索噪声\n",
    "            action = dist.sample().numpy()[0]  # 去掉batch维度\n",
    "            log_prob = dist.log_prob(torch.FloatTensor(action)).sum()\n",
    "\n",
    "            # 执行动作\n",
    "            next_state, reward, done, highest_return, _ = env.step(action)\n",
    "\n",
    "            # 存储经验\n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "            episode_dones.append(done)\n",
    "            episode_values.append(value.item())\n",
    "            episode_log_probs.append(log_prob.item())\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # 计算性能指标\n",
    "        returns = np.array(episode_rewards)\n",
    "        avg_reward = np.mean(returns)\n",
    "        total_return = (env.portfolio_value - env.init_balance) / env.init_balance\n",
    "        sharpe_ratio = np.mean(returns) / (np.std(returns) + 1e-6) * np.sqrt(252)\n",
    "\n",
    "        # 更新策略\n",
    "        if len(episode_states) >= batch_size:\n",
    "            agent.update(episode_states, episode_actions, episode_rewards, episode_dones)\n",
    "\n",
    "        # 记录训练过程\n",
    "        training_logs[\"episode\"].append(ep)\n",
    "        training_logs[\"avg_reward\"].append(avg_reward)\n",
    "        training_logs[\"total_return\"].append(total_return)\n",
    "        training_logs[\"max_drawdown\"].append(env.max_drawdown)\n",
    "        training_logs[\"sharpe_ratio\"].append(sharpe_ratio)\n",
    "\n",
    "        training_logs[\"highest_return\"].append(highest_return)  # for debugging\n",
    "\n",
    "        # 绘制持仓信息\n",
    "        if ep % 50 == 0 or ep == episodes - 1:\n",
    "            rb_positions = [state[-7] * 100 for state in episode_states]  # 恢复原始持仓信息\n",
    "            hc_positions = [state[-6] * 100 for state in episode_states]  # 恢复原始持仓信息\n",
    "            print(rb_positions)\n",
    "            # plt.figure(figsize=(15, 5))\n",
    "            # plt.plot(rb_positions, label=\"RB Position\")\n",
    "            # plt.plot(hc_positions, label=\"HC Position\")\n",
    "            # plt.title(f\"Episode {ep} - RB and HC Positions\")\n",
    "            # plt.xlabel(\"Step\")\n",
    "            # plt.ylabel(\"Position\")\n",
    "            # plt.legend()\n",
    "            # plt.show()\n",
    "\n",
    "        # 打印训练进度\n",
    "        if ep % 50 == 0 or ep == episodes - 1:\n",
    "            print(\n",
    "                f\"Episode {ep} | \"\n",
    "                f\"Avg Reward: {avg_reward:.2f} | \"\n",
    "                f\"Total Return: {total_return*100:.1f}% | \"\n",
    "                f\"Max Drawdown: {env.max_drawdown:.1f} | \"\n",
    "                f\"highest_return: {highest_return*100:.1f}% | \"\n",
    "                f\"Sharpe Ratio: {sharpe_ratio:.2f}\"\n",
    "            )\n",
    "\n",
    "    # 训练结果可视化\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(training_logs[\"episode\"], training_logs[\"avg_reward\"])\n",
    "    plt.title(\"Average Reward per Episode\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Avg Reward\")\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(training_logs[\"episode\"], np.array(training_logs[\"total_return\"]) * 100)\n",
    "    plt.title(\"Total Return (%)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Return (%)\")\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(training_logs[\"episode\"], np.array(training_logs[\"max_drawdown\"]) * 100)\n",
    "    plt.title(\"Max Drawdown (%)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Drawdown (%)\")\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(training_logs[\"episode\"], training_logs[\"sharpe_ratio\"])\n",
    "    plt.title(\"Sharpe Ratio\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Sharpe Ratio\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return env, agent, training_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d02de64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\gitrepo\\FICC_QIS\\venv\\Lib\\site-packages\\rqdatac\\client.py:257: UserWarning: Your account will be expired after  8 days. Please call us at 0755-22676337 to upgrade or purchase or renew your contract.\n",
      "  warnings.warn(\"Your account will be expired after  {} days. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "AbstractPathEffect.__init__() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env, agent, training_logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 87\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state_seq)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# 添加batch维度\u001b[39;00m\n\u001b[0;32m     86\u001b[0m action_mean, value, hidden \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mpolicy(state_tensor, hidden)\n\u001b[1;32m---> 87\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_mean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 添加探索噪声\u001b[39;00m\n\u001b[0;32m     88\u001b[0m action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# 去掉batch维度\u001b[39;00m\n\u001b[0;32m     89\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(torch\u001b[38;5;241m.\u001b[39mFloatTensor(action))\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[1;31mTypeError\u001b[0m: AbstractPathEffect.__init__() takes from 1 to 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "env, agent, training_logs = train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
